<|"Notebook" -> <|"Controller" -> "dc40e381-c444-4e85-bfaf-a466a7828215", 
   "FocusedCell" -> CoffeeLiqueur`Notebook`Cells`CellObj[
     CoffeeLiqueur`Notebook`Cells`CellObj`$3611], 
   "MessangerChannel" -> Messanger, "ModalsChannel" -> 
    "a9f4643a-1b02-4276-b5b0-e4e70fc329a7", "Objects" -> <||>, 
   "Path" -> "/Users/kirill/Github/wljs-demos-archive/Demos/06 - Realtime \
data capture/Microphone capture.wln", "ReadOnly" -> True, 
   "TOC" -> {CoffeeLiqueur`Extensions`TOC`Private`heading[1, "Audio stream", 
      CoffeeLiqueur`Notebook`Cells`CellObj[
       CoffeeLiqueur`Notebook`Cells`CellObj`$3604]], 
     CoffeeLiqueur`Extensions`TOC`Private`heading[2, "Javascript", 
      CoffeeLiqueur`Notebook`Cells`CellObj[
       CoffeeLiqueur`Notebook`Cells`CellObj`$3606]], 
     CoffeeLiqueur`Extensions`TOC`Private`heading[2, "Wolfram Language", 
      CoffeeLiqueur`Notebook`Cells`CellObj[
       CoffeeLiqueur`Notebook`Cells`CellObj`$3610]]}|>, 
 "Cells" -> {<|"Data" -> ".md\n# Audio stream\nVisualize the waveform", 
    "Display" -> "codemirror", "Hash" -> 
     "8aac9284-a61f-45a1-b772-e4ec11d4c14a", "Invisible" -> False, 
    "MetaOnly" -> False, "Props" -> <|"Hidden" -> True|>, "State" -> "Idle", 
    "Type" -> "Input", "UID" -> Null, "Notebook" -> 
     "a7a11d2e-d8fa-42ee-a3f2-ee19200051b3"|>, 
   <|"Data" -> "# Audio stream\nVisualize the waveform", 
    "Display" -> "markdown", "Hash" -> 
     "34d32b90-d382-4c2f-8bc9-e429a67a7718", "Invisible" -> False, 
    "MetaOnly" -> False, "Props" -> <||>, "State" -> "Idle", 
    "Type" -> "Output", "UID" -> Null, "Notebook" -> 
     "a7a11d2e-d8fa-42ee-a3f2-ee19200051b3"|>, 
   <|"Data" -> 
     ".md\n## Javascript\nGrab the data from AudioContext and send to WL", 
    "Display" -> "codemirror", "Hash" -> 
     "99a4ec0b-54f8-4619-a291-e77ef97743d7", "Invisible" -> False, 
    "MetaOnly" -> False, "Props" -> <|"Hidden" -> True|>, "State" -> "Idle", 
    "Type" -> "Input", "UID" -> Null, "Notebook" -> 
     "a7a11d2e-d8fa-42ee-a3f2-ee19200051b3"|>, 
   <|"Data" -> "## Javascript\nGrab the data from AudioContext and send to \
WL", "Display" -> "markdown", "Hash" -> 
     "262bb7d1-c9dc-4b1a-b6b1-d3e5604b564e", "Invisible" -> False, 
    "MetaOnly" -> False, "Props" -> <||>, "State" -> "Idle", 
    "Type" -> "Output", "UID" -> Null, "Notebook" -> 
     "a7a11d2e-d8fa-42ee-a3f2-ee19200051b3"|>, 
   <|"Data" -> ".js\n\n  // Constants\n  const audioContext = new \
AudioContext();\n  const analyser = audioContext.createAnalyser();\n  const \
scriptProcessor = audioContext.createScriptProcessor(2048, 1, 1);\n  const \
chunks = [];\n\n  // Variables\n  let stream = null;\n  let input = null;\n  \
let recorder = null;\n  let recording = null;\n  let isRecording = true;\n  \
let isPlaying = false;\n\n  // Setup analyser node\n  \
analyser.smoothingTimeConstant = 0.3;\n  analyser.fftSize = 1024;\n  \n  // \
Request access to the user's microphone.\n  const requestMicrophoneAccess = \
() => { \n    if (navigator.mediaDevices) {\n      \
navigator.mediaDevices.getUserMedia({audio: true}).then(stream => {\n        \
setAudioStream(stream);\n      }, error => {\n        alert('Something went \
wrong requesting the userMedia. <br/>Please make sure you\\'re viewing this \
demo over https.');\n      });\n    } else {\n      alert('Your browser does \
not support navigator.mediadevices. <br/>This is needed for this demo to \
work. Please try again in a differen browser.');\n    }  \n  }\n\n  // Set \
all variables which needed the audio stream\n  const setAudioStream = stream \
=> {\n    stream = stream;\n    input = \
audioContext.createMediaStreamSource(stream);\n    \n    \
input.connect(analyser);\n    analyser.connect(scriptProcessor);\n    \
scriptProcessor.connect(audioContext.destination);\n    \
scriptProcessor.onaudioprocess = processInput;\n  };\n\n\n  // Process the \
microphone input\n  const processInput = audioProcessingEvent => {\n      \
const array = new Uint8Array(analyser.frequencyBinCount);\n      \
analyser.getByteTimeDomainData(array);\n\n      \
server.kernel.io.fire('audio', Array.from(array));\n      \
//bars.push(getAverageVolume(array));\n  }  \n  \n  // Start the \
application\n\nlet state = false;\n\ncore.MicStart = async () => {\n  if \
(state) return;\n  state = true;\n  requestMicrophoneAccess(); \n  sign.style \
= \"color: red\";\n  sign.innerText = \"Recording...\";\n}\n\ncore.MicStop = \
async () => {\n  state = false;\n  input.disconnect();\n  \
analyser.disconnect();\n  scriptProcessor.disconnect();\n  \n  sign.style = \
\"color: blue\";\n  sign.innerText = \"Stopped\";\n}\n\nthis.ondestroy = () \
=> {\n  if (!state) return;\n  input.disconnect();\n  \
analyser.disconnect();\n  scriptProcessor.disconnect();\n};\n\nconst sign = \
document.createElement('div');\nsign.style = \"color: gray\";\nsign.innerText \
= \"Idle\";\n\nreturn sign;", "Display" -> "codemirror", 
    "Hash" -> "8d5684b7-796b-47ba-a0db-9fa83b59c2a9", "Invisible" -> False, 
    "MetaOnly" -> False, "Props" -> <|"Fade" -> True|>, "State" -> "Idle", 
    "Type" -> "Input", "UID" -> Null, "Notebook" -> 
     "a7a11d2e-d8fa-42ee-a3f2-ee19200051b3"|>, 
   <|"Data" -> "\n  // Constants\n  const audioContext = new \
AudioContext();\n  const analyser = audioContext.createAnalyser();\n  const \
scriptProcessor = audioContext.createScriptProcessor(2048, 1, 1);\n  const \
chunks = [];\n\n  // Variables\n  let stream = null;\n  let input = null;\n  \
let recorder = null;\n  let recording = null;\n  let isRecording = true;\n  \
let isPlaying = false;\n\n  // Setup analyser node\n  \
analyser.smoothingTimeConstant = 0.3;\n  analyser.fftSize = 1024;\n  \n  // \
Request access to the user's microphone.\n  const requestMicrophoneAccess = \
() => { \n    if (navigator.mediaDevices) {\n      \
navigator.mediaDevices.getUserMedia({audio: true}).then(stream => {\n        \
setAudioStream(stream);\n      }, error => {\n        alert('Something went \
wrong requesting the userMedia. <br/>Please make sure you\\'re viewing this \
demo over https.');\n      });\n    } else {\n      alert('Your browser does \
not support navigator.mediadevices. <br/>This is needed for this demo to \
work. Please try again in a differen browser.');\n    }  \n  }\n\n  // Set \
all variables which needed the audio stream\n  const setAudioStream = stream \
=> {\n    stream = stream;\n    input = \
audioContext.createMediaStreamSource(stream);\n    \n    \
input.connect(analyser);\n    analyser.connect(scriptProcessor);\n    \
scriptProcessor.connect(audioContext.destination);\n    \
scriptProcessor.onaudioprocess = processInput;\n  };\n\n\n  // Process the \
microphone input\n  const processInput = audioProcessingEvent => {\n      \
const array = new Uint8Array(analyser.frequencyBinCount);\n      \
analyser.getByteTimeDomainData(array);\n\n      \
server.kernel.io.fire('audio', Array.from(array));\n      \
//bars.push(getAverageVolume(array));\n  }  \n  \n  // Start the \
application\n\nlet state = false;\n\ncore.MicStart = async () => {\n  if \
(state) return;\n  state = true;\n  requestMicrophoneAccess(); \n  sign.style \
= \"color: red\";\n  sign.innerText = \"Recording...\";\n}\n\ncore.MicStop = \
async () => {\n  state = false;\n  input.disconnect();\n  \
analyser.disconnect();\n  scriptProcessor.disconnect();\n  \n  sign.style = \
\"color: blue\";\n  sign.innerText = \"Stopped\";\n}\n\nthis.ondestroy = () \
=> {\n  if (!state) return;\n  input.disconnect();\n  \
analyser.disconnect();\n  scriptProcessor.disconnect();\n};\n\nconst sign = \
document.createElement('div');\nsign.style = \"color: gray\";\nsign.innerText \
= \"Idle\";\n\nreturn sign;", "Display" -> "js", 
    "Hash" -> "0723ae96-741d-491b-86fa-ac04e28d6f2a", "Invisible" -> False, 
    "MetaOnly" -> False, "Props" -> <||>, "State" -> "Idle", 
    "Type" -> "Output", "UID" -> Null, "Notebook" -> 
     "a7a11d2e-d8fa-42ee-a3f2-ee19200051b3"|>, 
   <|"Data" -> ".md\n## Wolfram Language\nReceive and draw", 
    "Display" -> "codemirror", "Hash" -> 
     "223f2aa1-9ade-4c8d-8ed2-741a88510213", "Invisible" -> False, 
    "MetaOnly" -> False, "Props" -> <|"Hidden" -> True|>, "State" -> "Idle", 
    "Type" -> "Input", "UID" -> Null, "Notebook" -> 
     "a7a11d2e-d8fa-42ee-a3f2-ee19200051b3"|>, 
   <|"Data" -> "## Wolfram Language\nReceive and draw", 
    "Display" -> "markdown", "Hash" -> 
     "63b2078c-3dfd-444d-a65d-a59a5dfbed59", "Invisible" -> False, 
    "MetaOnly" -> False, "Props" -> <||>, "State" -> "Idle", 
    "Type" -> "Output", "UID" -> Null, "Notebook" -> 
     "a7a11d2e-d8fa-42ee-a3f2-ee19200051b3"|>, 
   <|"Data" -> "LeakyModule[{line = {{0.,-256.}, {512.,256.}}},\n  \
EventHandler[\"audio\", Function[data,\n    \n    line = MapIndexed[{#2[[1]], \
#1}&, data];\n  ]];\n  Graphics[\n    Line[line // Offload], \n    PlotRange \
-> {{0,512}, {127-50, 127+50}},\n    \n    \"TransitionDuration\"->30, \n    \
\"TransitionType\"->\"Linear\"\n  ]\n]\n\n\n  \
EventHandler[InputButton[\"Start\"], Function[Null, \
FrontSubmit[MicStart[]]]]\n\n  EventHandler[InputButton[\"Stop\"], \
Function[Null, FrontSubmit[MicStop[]]]]\n", "Display" -> "codemirror", 
    "Hash" -> "70603448-41d5-41bc-8a10-2c65ffce9ce5", "Invisible" -> False, 
    "MetaOnly" -> False, "Props" -> <||>, "State" -> "Idle", 
    "Type" -> "Input", "UID" -> Null, "Notebook" -> 
     "a7a11d2e-d8fa-42ee-a3f2-ee19200051b3"|>}, "serializer" -> "jsfn4"|>
